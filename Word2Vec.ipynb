{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc177dd0-8b4c-4297-986b-1f429fc4cde8",
   "metadata": {},
   "source": [
    "## Installing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b536102d-08d2-46ad-b428-57eba977e271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.12/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /opt/anaconda3/lib/python3.12/site-packages (from gensim) (1.26.0)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.12/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: portalocker in /opt/anaconda3/lib/python3.12/site-packages (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install portalocker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b92fe70-e118-49d4-8ab3-fa5abab96512",
   "metadata": {},
   "source": [
    "## Importing required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f7fba85-02da-418c-b64a-1706f2b6dfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tinonturjamajumder/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/tinonturjamajumder/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import warnings\n",
    "def warn(*args,**kwargs):\n",
    "    pass\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd7a088-543a-49e9-84cd-ec1f72534032",
   "metadata": {},
   "source": [
    "## Newly adapted import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5e78669-e768-435f-be51-8da5ffbb4399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from gensim.models import Word2Vec\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from IPython.core.display import display,SVG\n",
    "\n",
    "from torchtext.vocab import GloVe,vocab\n",
    "from torchdata.datapipes.iter import IterableWrapper,Mapper\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd50705-26ff-47b5-a647-c94fa847537b",
   "metadata": {},
   "source": [
    "Define a function to plot word embeddings in a 2d space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "483ff1f5-de3c-4343-bf29-4c7ae997aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(word_embeddings,vocab = vocab):\n",
    "    tsne = TSNE(n_components = 2,random_state = 0)\n",
    "    word_embeddings_2d = tsne.fit_transform(word_embeddings)\n",
    "\n",
    "\n",
    "    # Plotting the results with labels from vocab\n",
    "    plt.figure(figsize = (15,15))\n",
    "\n",
    "    for i, word in enumerate(vocab.get_itos()): # assuming vocab.itos gives the list of words in your vocab\n",
    "        plt.scatter(word_embeddings_2d[i,0],word_embeddings_2d[i,1])\n",
    "        plt.annotate(word, (word_embeddings_2d[i,0], word_embeddings_2d[i,1]))\n",
    "\n",
    "    plt.xlabel(\"t-SNE component 1\")\n",
    "    plt.ylabel(\"t-SNE component 2\")\n",
    "    plt.title(\"Word Embeddings visualized with t-SNE\")\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972c17aa-80d9-4017-b351-f805cc820df4",
   "metadata": {},
   "source": [
    "Define a function that returns similar words to a specific word by calculating Cosine Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a55e9a9-37ac-4fb0-a764-0f3ea8a37488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function returns the most similar words to a target word by calculating word vectors cosine distance\n",
    "\n",
    "def find_simiar_words(word,word_embeddings,top_k = 5):\n",
    "    if word not in word_embeddings:\n",
    "        print(\"Word not found in embeddings\")\n",
    "\n",
    "        return []\n",
    "\n",
    "    # Get the embedding for the given word\n",
    "    target_embedding = word_embeddings[word]\n",
    "\n",
    "    # Calculate cosine similarities between the target word and all other words\n",
    "    similarities = {}\n",
    "\n",
    "    for w, embedding in word_embeddings.items():\n",
    "        if w != word:\n",
    "            similarity = torch.dot(target_embedding,embedding) / (\n",
    "                torch.norm(target_embedding) * torch.norm(embedding)\n",
    "            )\n",
    "\n",
    "            similarities[w] = similarity.item()\n",
    "\n",
    "    # sort the similarities in descending order\n",
    "    sorted_similarities = sorted(similarities.items(),key = lambda x: x[1],reverse = True)\n",
    "\n",
    "\n",
    "    # Return the top k similar words\n",
    "    most_similar_words = [w for w,_ in sorted_similarities[:top_k]]\n",
    "\n",
    "    return most_similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f5b714-b925-4ff6-89d7-c0d1cc64b6eb",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6e6b745-497d-4a3e-a2af-870cddbc04af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model,dataloader,loss_fn, optimizer, num_epochs = 100):\n",
    "\n",
    "    \"\"\"Train the model for the specified number of epochs\n",
    "    \n",
    "    Args:\n",
    "        model: the pytorch model to be trained.\n",
    "        dataloader: DataLoader provided data for training\n",
    "        loss_fn: Loss Function,\n",
    "        optimizer: optimizer for updating model's weights\n",
    "        num_epochs: number of epochs to train the model for\n",
    "\n",
    "    Returns:\n",
    "        model: The trainded model\n",
    "        epoch_losses: list of average losses for each epoch\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # List to store running loss\n",
    "    loss_list = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "\n",
    "        # Storing running loss values for the current epoch\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # using tqdm for a progress bar\n",
    "        for idx, samples in enumerate(dataloader):\n",
    "            # 0. Optimizer\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Check for embedding_bag layer in the model\n",
    "            if any(isinstance(module,nn.EmbeddingBag) for _,module in model.named_modules()):\n",
    "                target, context, offsets = samples\n",
    "\n",
    "                # 1. Forward pass\n",
    "                predicted = model(context, offsets)\n",
    "\n",
    "            # check for embedding layer in the model\n",
    "            elif any(isinstance(module,nn.Embedding) for _,module in model.named_modules()):\n",
    "                target,context = samples\n",
    "\n",
    "                # 1. Forward Pass\n",
    "                predicted  = model(context)\n",
    "\n",
    "            # 2. Loss Calculation\n",
    "\n",
    "            loss = loss_fn(predicted,target)\n",
    "\n",
    "            # 3. backward Propagation\n",
    "            loss.backward()\n",
    "\n",
    "            # 4. Gradient clipping is a way to limit (clip) the gradientsâ€™ magnitude, ensuring they stay within a reasonable range.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),0.1)\n",
    "\n",
    "            # 5. optimizer step\n",
    "            optimizer.step()\n",
    "\n",
    "            # 6. accumulate the loss\n",
    "            running_loss +=loss.item()\n",
    "\n",
    "\n",
    "        # Append average loss for the epoch\n",
    "        loss_list.append(running_loss/len(dataloader))\n",
    "\n",
    "    return model,loss_list\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88604aaf-08e5-4dd8-833f-4e87a80174b9",
   "metadata": {},
   "source": [
    "## Create and train Word2Vec models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1354a62-4adb-4245-8325-b1f44b7d465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data = \"\"\"I wish I was little bit taller\n",
    "I wish I was a baller\n",
    "She wore a small black dress to the party\n",
    "The dog chased a big red ball in the park\n",
    "He had a huge smile on his face when he won the race\n",
    "The tiny kitten played with a fluffy toy mouse\n",
    "The team celebrated their victory with a grand parade\n",
    "She bought a small, delicate necklace for her sister\n",
    "The mountain peak stood majestic and tall against the clear blue sky\n",
    "The toddler took small, careful steps as she learned to walk\n",
    "The house had a spacious backyard with a big swimming pool\n",
    "He felt a sense of accomplishment after completing the challenging puzzle\n",
    "The chef prepared a delicious, flavorful dish using fresh ingredients\n",
    "The children played happily in the small, cozy room\n",
    "The book had an enormous impact on readers around the world\n",
    "The wind blew gently, rustling the leaves of the tall trees\n",
    "She painted a beautiful, intricate design on the small canvas\n",
    "The concert hall was filled with thousands of excited fans\n",
    "The garden was adorned with colorful flowers of all sizes\n",
    "I hope to achieve great success in my chosen career path\n",
    "The skyscraper towered above the city, casting a long shadow\n",
    "He gazed in awe at the breathtaking view from the mountaintop\n",
    "The artist created a stunning masterpiece with bold brushstrokes\n",
    "The baby took her first steps, a small milestone that brought joy to her parents\n",
    "The team put in a tremendous amount of effort to win the championship\n",
    "The sun set behind the horizon, painting the sky in vibrant colors\n",
    "The professor gave a fascinating lecture on the history of ancient civilizations\n",
    "The house was filled with laughter and the sound of children playing\n",
    "She received a warm, enthusiastic welcome from the audience\n",
    "The marathon runner had incredible endurance and determination\n",
    "The child's eyes sparkled with excitement upon opening the gift\n",
    "The ship sailed across the vast ocean, guided by the stars\n",
    "The company achieved remarkable growth in a short period of time\n",
    "The team worked together harmoniously to complete the project\n",
    "The puppy wagged its tail, expressing its happiness and affection\n",
    "She wore a stunning gown that made her feel like a princess\n",
    "The building had a grand entrance with towering columns\n",
    "The concert was a roaring success, with the crowd cheering and clapping\n",
    "The baby took a tiny bite of the sweet, juicy fruit\n",
    "The athlete broke a new record, achieving a significant milestone in her career\n",
    "The sculpture was a masterpiece of intricate details and craftsmanship\n",
    "The forest was filled with towering trees, creating a sense of serenity\n",
    "The children built a small sandcastle on the beach, their imaginations running wild\n",
    "The mountain range stretched as far as the eye could see, majestic and awe-inspiring\n",
    "The artist's brush glided smoothly across the canvas, creating a beautiful painting\n",
    "She received a small token of appreciation for her hard work and dedication\n",
    "The orchestra played a magnificent symphony that moved the audience to tears\n",
    "The flower bloomed in vibrant colors, attracting butterflies and bees\n",
    "The team celebrated their victory with a big, extravagant party\n",
    "The child's laughter echoed through the small room, filling it with joy\n",
    "The sunflower stood tall, reaching for the sky with its bright yellow petals\n",
    "The city skyline was dominated by tall buildings and skyscrapers\n",
    "The cake was adorned with a beautiful, elaborate design for the special occasion\n",
    "The storm brought heavy rain and strong winds, causing widespread damage\n",
    "The small boat sailed peacefully on the calm, glassy lake\n",
    "The artist used bold strokes of color to create a striking and vivid painting\n",
    "The couple shared a passionate kiss under the starry night sky\n",
    "The mountain climber reached the summit after a long and arduous journey\n",
    "The child's eyes widened in amazement as the magician performed his tricks\n",
    "The garden was filled with the sweet fragrance of blooming flowers\n",
    "The basketball player made a big jump and scored a spectacular slam dunk\n",
    "The cat pounced on a small mouse, displaying its hunting instincts\n",
    "The mansion had a grand entrance with a sweeping staircase and chandeliers\n",
    "The raindrops fell gently, creating a rhythmic patter on the roof\n",
    "The baby took a big step forward, encouraged by her parents' applause\n",
    "The actor delivered a powerful and emotional performance on stage\n",
    "The butterfly fluttered its delicate wings, mesmerizing those who watched\n",
    "The company launched a small-scale advertising campaign to test the market\n",
    "The building was constructed with strong, sturdy materials to withstand earthquakes\n",
    "The singer's voice was powerful and resonated throughout the concert hall\n",
    "The child built a massive sandcastle with towers, moats, and bridges\n",
    "The garden was teeming with a variety of small insects and buzzing bees\n",
    "The athlete's muscles were well-developed and strong from years of training\n",
    "The sun cast long shadows as it set behind the mountains\n",
    "The couple exchanged heartfelt vows in a beautiful, intimate ceremony\n",
    "The dog wagged its tail vigorously, a sign of excitement and happiness\n",
    "The baby let out a tiny giggle, bringing joy to everyone around\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d18dad9-8db2-47ec-a626-e9f0be525433",
   "metadata": {},
   "source": [
    "Now prepare the data by tokenizing it and made it into vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d41a49fc-28a0-4704-8547-acf2b795d2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "tokenized_toy_data = tokenizer(toy_data)\n",
    "\n",
    "vocab = build_vocab_from_iterator(map(tokenizer,toy_data.split()),specials = [\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02cd3741-6a60-479c-944f-f3c3c05fd77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_toy_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed496a8-8425-4a18-9272-d3e2c4e939c0",
   "metadata": {},
   "source": [
    "Lets check how a sentence looks like after tokenization and numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55f15337-5ca1-4a6a-915e-a5b4d2424a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'wish', 'i', 'was', 'a', 'baller']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "sample_sentence = \"I wish I was a baller\"\n",
    "tokenized_sample = tokenizer(sample_sentence)\n",
    "tokenized_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e098b07-0650-494d-9b6b-f1a9ede8d18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 108, 20, 7, 2, 133]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_list = [vocab[words] for words in tokenized_sample]\n",
    "number_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c61e24-5986-441f-a2e2-2ec91613f8e5",
   "metadata": {},
   "source": [
    "write a function to apply numericalization on all tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d63303ab-7aa2-4772-8ab7-166ac30264e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda tokens: [vocab[token] for token in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c62beef-d419-4cb7-815c-9c9e1bbcdb15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 108, 272, 136]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c3a961-117e-4d6e-a5b2-c0c37218d8de",
   "metadata": {},
   "source": [
    "## Continuous Bag of Words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd4e969-2e6d-40c8-8e38-7231cd434b3c",
   "metadata": {},
   "source": [
    "For the continuous bag of words model, use a 'context' to predict a target word. The `context` is typically a set of surrounding words. For example, if your context window is of size 2, then you take two words before and two words after the target words as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa09b41b-39bc-4f32-a305-4d788bdc725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "\n",
    "cbow_data = []\n",
    "\n",
    "# modified code\n",
    "\n",
    "for i in range(CONTEXT_SIZE,len(tokenized_toy_data)-CONTEXT_SIZE):\n",
    "\n",
    "    # context word\n",
    "    cbow_data.append((([tokenized_toy_data[i-j-1] for j in range(CONTEXT_SIZE)]+[tokenized_toy_data[i+j+1] for j in range(CONTEXT_SIZE)]),\n",
    "    tokenized_toy_data[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b507a792-3dcf-4532-a1a8-cf88ab9de6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cbow_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60f4c479-0f6d-4cc1-985f-e4e5e63e2d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "890"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cbow_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6595d60-1186-44f5-b451-6da149f3620e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['wish', 'i', 'was', 'little'], 'i'),\n",
       " (['i', 'wish', 'little', 'bit'], 'was'),\n",
       " (['was', 'i', 'bit', 'taller'], 'little'),\n",
       " (['little', 'was', 'taller', 'i'], 'bit'),\n",
       " (['bit', 'little', 'i', 'wish'], 'taller'),\n",
       " (['taller', 'bit', 'wish', 'i'], 'i'),\n",
       " (['i', 'taller', 'i', 'was'], 'wish'),\n",
       " (['wish', 'i', 'was', 'a'], 'i'),\n",
       " (['i', 'wish', 'a', 'baller'], 'was'),\n",
       " (['was', 'i', 'baller', 'she'], 'a')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "275f23bb-a813-451f-ab8d-6d88000f95ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([20, 108, 272, 136], 7)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context,target = cbow_data[1]\n",
    "context_vocab= vocab(context)\n",
    "target_vocab= vocab[target]\n",
    "context_vocab,target_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6c8276-c207-4e6e-b338-3b806e044822",
   "metadata": {},
   "source": [
    "## Set up the collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36991b26-88e4-42ff-9732-28d729dd5a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_function(batch):\n",
    "\n",
    "    \"\"\"Collate function will take tokens into account and convert it into vocab tensor\"\"\"\n",
    "\n",
    "    target_list, context_list, offsets = [],[],[0]\n",
    "\n",
    "    for context,target in batch:\n",
    "        \n",
    "        target_list.append(vocab[target])\n",
    "        processed_context = torch.tensor(text_pipeline(context),dtype = torch.int64)\n",
    "        context_list.append(processed_context)\n",
    "        # context_list.append(vocab(context)) # It will work fine if the context data comes as separate word\n",
    "        target_list.append(vocab[target])\n",
    "        offsets.append(processed_context.shape[0])\n",
    "\n",
    "    target_list = torch.tensor(target_list)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    context_list  = torch.cat(context_list)\n",
    "    return target_list,context_list,offsets\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32414a49-13b8-4b25-bd14-b62495fd75f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_list(Tokenized target words): tensor([ 20,  20,   7,   7, 272, 272, 136, 136, 376, 376,  20,  20, 108, 108,\n",
      "         20,  20,   7,   7,   2,   2]) , \n",
      "context_list(Surrounding context words): tensor([108,  20,   7, 272,  20, 108, 272, 136,   7,  20, 136, 376, 272,   7,\n",
      "        376,  20, 136, 272,  20, 108, 376, 136, 108,  20,  20, 376,  20,   7,\n",
      "        108,  20,   7,   2,  20, 108,   2, 133,   7,  20, 133,  14]) , \n",
      "offsets(Starting indexes of context words for each target): tensor([ 0,  4,  8, 12, 16, 20, 24, 28, 32, 36]) \n"
     ]
    }
   ],
   "source": [
    "target_list, context_list,offsets = collate_function(cbow_data[:10])\n",
    "print(f\"target_list(Tokenized target words): {target_list} , \\ncontext_list(Surrounding context words): {context_list} , \\noffsets(Starting indexes of context words for each target): {offsets} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af2913b7-75a7-4f5e-8e8b-4ced1a85b68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x1506ad6d0>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE  = 64\n",
    "\n",
    "# the dataset should be a list, which consists of a tuple. one is context, and another one is target\n",
    "dataloader_cbow = DataLoader(dataset=cbow_data,\n",
    "                            batch_size = BATCH_SIZE,\n",
    "                            shuffle = True,\n",
    "                            collate_fn = collate_function)\n",
    "\n",
    "print(dataloader_cbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7b660e-67a9-466a-98f0-4a31bfafd39f",
   "metadata": {},
   "source": [
    "The CBOW model shown here starts with an embeddingbag layer, which takes a variable length list of context word indices and produces an averaged embedding of size embed_dim. This embedding is then passed through a linera layer that reduces its dimension to embed_dim/2. After applying a ReLU activaiton, the output is processed by another linear layer, transforming it to match the vocabulary size, thus allowing the model to predict the probabiltiy of any word form the vocabulary as the target word, The overall flow moves from contextual words indices to predicting the central word in the Continuous Bag of Words approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745f9727-d681-4f0c-9370-3917e5bf0310",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    # Initialize the CBOW model\n",
    "    def __init__(self,vocab_size, embed_dim, num_class):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the embedding layer using nn.EmbeddingBag\n",
    "        # It outputs the average of context words embeddings\n",
    "\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim)\n",
    "\n",
    "        # Define the first linear layer with input size embed_dim and output size embed_dim/2\n",
    "\n",
    "        self.linear1 = nn.Linear(embed_dim,embed_dim/2)\n",
    "\n",
    "        # Define the fully connected layer with input size embed_dim/2 and output size vocab_size\n",
    "        self.fc = nn.Linear(embed_dim/2,vocab_size)\n",
    "\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weight(self):\n",
    "        # initialize the weights of the embedding layer\n",
    "        initrange = 0.5\n",
    "\n",
    "        self.embedding.weight.data.uniform_(-initrange,initrange)\n",
    "\n",
    "        # initialize the weights of the fully connected layer\n",
    "        self.fc.weight.data.uniform_(-initrange,initrange)\n",
    "\n",
    "        # initialize the biases of the fully connected layer\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self,text,offsets):\n",
    "\n",
    "        # pass the input text and offsets throught the embedding layer\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
