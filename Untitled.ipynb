{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4364606b-091e-434f-bfab-694bedd59877",
   "metadata": {},
   "source": [
    "# STEPS:\n",
    "\n",
    "    1. Import Libraries\n",
    "    2. Take the corpus in account from where vocabulary will be created\n",
    "    3. Tokenize the word of the corpus\n",
    "        * Cleaning the token. (Removing any excess element from the word)\n",
    "        * \n",
    "\n",
    "    4. Create vocabulary from the tokenize words (take account of the indices of the word)\n",
    "    5. Create Embedding Layers from the vocab\n",
    "    6. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e30eb6-1b69-4d1c-b2c8-25afb0ada6aa",
   "metadata": {},
   "source": [
    "## Import required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "31b6ec4d-ca30-4f33-a6ea-44858302313f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tinonturjamajumder/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/tinonturjamajumder/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "import warnings\n",
    "def warn(*args,**kwargs):\n",
    "    pass\n",
    "warnings.warn  = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5cc231-e740-4dff-974c-56d9348f5ac9",
   "metadata": {},
   "source": [
    "## Data Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f0558191-21b0-46c0-a33f-f68f4ff759d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "song= \"\"\"We are no strangers to love\n",
    "You know the rules and so do I\n",
    "A full commitments what Im thinking of\n",
    "You wouldnt get this from any other guy\n",
    "I just wanna tell you how Im feeling\n",
    "Gotta make you understand\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Weve known each other for so long\n",
    "Your hearts been aching but youre too shy to say it\n",
    "Inside we both know whats been going on\n",
    "We know the game and were gonna play it\n",
    "And if you ask me how Im feeling\n",
    "Dont tell me youre too blind to see\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Weve known each other for so long\n",
    "Your hearts been aching but youre too shy to say it\n",
    "Inside we both know whats been going on\n",
    "We know the game and were gonna play it\n",
    "I just wanna tell you how Im feeling\n",
    "Gotta make you understand\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9671528a-39fa-4481-bd91-a10e67f5dbb7",
   "metadata": {},
   "source": [
    "## Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5970fe68-8f3a-48ba-ab0b-29399cb36476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_string(s):\n",
    "\n",
    "    # remove anything but digit from string (digits,whitespaces and signs)\n",
    "    s = re.sub(r\"[^\\w\\s]\",'',s)\n",
    "\n",
    "    # remove one or more than whitespaces\n",
    "    s = re.sub(r\"\\s+\",'',s)\n",
    "\n",
    "    # remove digits from the token\n",
    "    s = re.sub(r\"\\d\", '', s)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "294d3b13-7adc-46a5-9048-8cfcade10d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MnTr'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'M@n06 T0r!'\n",
    "processed_text = preprocess_string(text)\n",
    "processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db79c80-bf0d-4264-b444-a4206915f3dc",
   "metadata": {},
   "source": [
    "## Token Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fddf03b3-1599-466c-9932-858519996b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_token(text,tokenizer):\n",
    "\n",
    "    token = tokenizer(text)\n",
    "\n",
    "    tokens = [preprocess_string(w).lower() for w in token if len(w) !=0 and w not in string.punctuation]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "609abd7f-dfcf-4902-ad46-b07516f9eef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', 'are', 'no', 'strangers', 'to', 'love', 'you', 'know', 'the', 'rules']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer instantiate\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "tokens = clear_token(song,tokenizer)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5db76e-9f8c-4226-b725-741f647acb95",
   "metadata": {},
   "source": [
    "## Vocabulary Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "df79b17b-4c28-4039-ae7d-5639c05d59d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(map(tokenizer,song.split()),specials = [\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fcdc6a88-c64d-4b6b-9fc7-fbcdcfd8a0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 58, 70, 74, 25, 69, 2, 20, 31, 72]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(tokens[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e041c-0086-4060-a7db-a7b76e595864",
   "metadata": {},
   "source": [
    "## Embedding Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cdcfd3ca-d72a-4030-af63-5e3c1758c62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genembedding(vocab,embedding_dim):\n",
    "\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "    embeddings = nn.Embedding(vocab_size,embedding_dim)\n",
    "\n",
    "    return embeddings    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c4a52694-2598-49df-9848-826bf9c0fd07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(79, 20)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = genembedding(vocab,20)\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d036cd12-c1c4-479f-a1de-6b60114e589d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1194, -0.9281, -0.8670, -0.5013, -0.3364,  0.5581, -0.4982, -1.2647,\n",
       "        -1.1862,  0.8875, -0.2482, -0.2053, -0.0668, -0.4051, -0.1475, -1.2057,\n",
       "         0.0125,  0.3150, -0.4275, -0.3477], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(torch.tensor(78))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2020304-5549-4c86-8eee-8c2610767a8a",
   "metadata": {},
   "source": [
    "## Generating Context-Target pairs (n-grams)\n",
    "\n",
    "Context embedding values should be feed to the model and target word should be predicted by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905f7d10-a6b6-4115-9822-ed52f11e86da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genngrams(tokens,context_size,vocab):\n",
    "    ngrams = [vocab(tokens[i-j-1]) for j in range(len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e315377c-2daf-4b6e-ae9a-695b9f1e7830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f45776d3-7b5b-42cc-b4ee-fc908996d379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[70]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab([tokens[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54b2d99-f4b2-48ee-a3a3-0de026e4bd3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
