{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6b94e44-459c-485b-a6fb-4fbcba6e4daf",
   "metadata": {},
   "source": [
    "# **Building and Training a Feedforward Neural Network for Language Modeling**\n",
    "\n",
    "Estimated time needed: **60** minutes\n",
    "\n",
    "This project explores the use of Feedforward Neural Networks (FNNs) in language modeling. The primary objective is to build a neural network that learns word relationships and generates meaningful text sequences. The implementation is done using PyTorch, covering key aspects of Natural Language Processing (NLP), such as:\n",
    "* Tokenization & Indexing: Converting text into numerical representations.\n",
    "* Embedding Layers: Mapping words to dense vector representations for efficient learning.\n",
    "* Context-Target Pair Generation (N-grams): Structuring training data for sequence prediction.\n",
    "* Multi-Class Neural Network: Designing a model to predict the next word in a sequence.\n",
    "\n",
    "The training process includes optimizing the model with loss functions and backpropagation techniques to improve accuracy and coherence in text generation. By the end of the project, you will have a working FNN-based language model capable of generating text sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a75d855-a3e7-468d-8d96-633f30b69857",
   "metadata": {},
   "source": [
    "## Import required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fbad259-44bf-429f-8db1-031781c5cbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tinonturjamajumder/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/tinonturjamajumder/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "import warnings\n",
    "def warn(*args,**kwargs):\n",
    "    pass\n",
    "warnings.warn = warn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9895fe0-fb45-459b-98a6-e4fddd8698d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "song= \"\"\"We are no strangers to love\n",
    "You know the rules and so do I\n",
    "A full commitments what Im thinking of\n",
    "You wouldnt get this from any other guy\n",
    "I just wanna tell you how Im feeling\n",
    "Gotta make you understand\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Weve known each other for so long\n",
    "Your hearts been aching but youre too shy to say it\n",
    "Inside we both know whats been going on\n",
    "We know the game and were gonna play it\n",
    "And if you ask me how Im feeling\n",
    "Dont tell me youre too blind to see\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Weve known each other for so long\n",
    "Your hearts been aching but youre too shy to say it\n",
    "Inside we both know whats been going on\n",
    "We know the game and were gonna play it\n",
    "I just wanna tell you how Im feeling\n",
    "Gotta make you understand\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6b403f7-b1d8-4a6e-8616-adcb3cb39aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_string(s):\n",
    "    \"\"\"\n",
    "    Preprocess a given string by performing the following steps:\n",
    "    1. Removing anything but letters and digits\n",
    "    2. Removing whitespace\n",
    "    3. Removes all numeric digits\n",
    "    \"\"\"\n",
    "\n",
    "    # Removing all non-word characters (everything except letters and numbers)\n",
    "    # \\w matches any word characters (letters, numbers, and underscores)\n",
    "    # \\s matches any whitespace characters\n",
    "    # ^ inside [] negates the selection, so [^\\w\\s] matches anything that's not a word character or whitespace\n",
    "    s = re.sub(r\"[^\\w\\s]\", '',s)\n",
    "\n",
    "    # removing white spaces\n",
    "    # \\s+ matches one or more whitespace characters.\n",
    "    s = re.sub(r\"\\s+\",'',s)\n",
    "\n",
    "    # removing digits\n",
    "    # \\d matches digits (0-9)\n",
    "    s = re.sub(r\"\\d\",'',s)\n",
    "\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a620950-9d39-4226-99db-32a0574381dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_words(word,tokenizer):\n",
    "    )_token if len(w) !=0 and w not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f5847c9-0c2a-44a1-879e-5b6c84165b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = preprocess_words(song,word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccd7e041-0baf-4b6e-b38b-be61f7eefa0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', 'are', 'no', 'strangers', 'to', 'love', 'you', 'know', 'the', 'rules']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b02e922-c84c-4189-ac2b-31c4da557fcd",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91b8dae0-7752-4c7f-868a-ab306f6650b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2a59402-21d5-4ed1-ac9b-b5ff7a854d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizetext(song):\n",
    "    \"\"\"Tokenizes the input text(song) and builds a vocabulary from the tokens\n",
    "\n",
    "    Steps:\n",
    "        1. Tokenizations: The function splits the input text into words and applies a tokenizer function to each word\n",
    "        2. vocabulary building: \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    tokenized_song = map(tokenizer,song.split())\n",
    "\n",
    "    vocab  = build_vocab_from_iterator(tokenized_song,specials = [\"<unk>\"])\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd2b690b-9489-4ee5-87e1-a288eb6d9547",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab= tokenizetext(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8fcf8ce9-b42d-4ed7-bc47-259badae2a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', 'gonna', 'you', 'never', 'and', 'tell', 'make', 'say', 'a', 'around']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab.get_itos())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b57f7ef-bcd6-4b16-a1a1-ce3182ed1256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['we', 'are', 'no', 'strangers', 'to', 'love', 'you', 'know', 'the', 'rules'],\n",
       " [21, 58, 70, 74, 25, 69, 2, 20, 31, 72])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:10],vocab(tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37980cf-1fdd-4d65-ae8e-9fab9e6f1e51",
   "metadata": {},
   "source": [
    "A function that converts raw text into indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1e4932c-5926-40af-89b7-39512a64284a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 58, 70, 74, 25, 69, 2, 20, 31, 72]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "text_pipeline(song)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b85c7e4-deb8-4fbb-9e8a-c29c6b3588c6",
   "metadata": {},
   "source": [
    "Convert `index` *-->* `words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0161462e-5261-4743-a450-bb13b332f3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gonna'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_token = vocab.get_itos()\n",
    "index_to_token[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8131cc71-162d-4f81-8fe7-1f5a969bdb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wouldnt',\n",
       " 'what',\n",
       " 'thinking',\n",
       " 'rules',\n",
       " 'no',\n",
       " 'love',\n",
       " 'if',\n",
       " 'full',\n",
       " 'from',\n",
       " 'get']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab.get_stoi())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9fcbfcf2-e04f-4f85-bbb1-89461f05c918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = vocab.get_stoi() # stoi is a dictionary basically\n",
    "val['gonna']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce6c9ac-3d77-4818-b2c8-87c166b43aff",
   "metadata": {},
   "source": [
    "## Embedding Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cb71ad31-1b71-4d5f-8063-39abe7ebe32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(vocab):\n",
    "    \"\"\" Generating an embedding layer for the given vocabulary,\n",
    "    The embedding layer transforms words into dense vector representation,\n",
    "    allowing the model to learn semantic relationship between words.\n",
    "\n",
    "    Parameters: vocab_size, embedding_dimension\n",
    "\n",
    "    output: nn.Embedding: A PyTorch embedding layer with a specified dimension\"\"\"\n",
    "\n",
    "    embedding_dimension = 20\n",
    "    embedding = nn.Embedding(len(vocab),embedding_dimension)\n",
    "    return embedding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "76ac61f9-5a12-4e63-b4b2-862fda639f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.sparse.Embedding"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embedding(vocab)\n",
    "\n",
    "type(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0cc28daf-83f3-4e42-837a-0634eb4a1704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0099, -0.6683, -2.2261,  0.9901, -0.5307, -0.9852, -0.9635,  0.9964,\n",
       "        -0.5094, -0.3424,  1.0937, -0.0086, -1.3811,  1.7139,  0.7446, -1.8681,\n",
       "         0.9568, -0.2175, -0.5106, -0.5009], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings(torch.tensor(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cf224ee8-1f8b-48ad-b56d-05d5f110f04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: <unk>\n",
      "index: 0\n",
      "embedding: tensor([-0.3025, -1.1382, -0.6952,  0.1230, -0.6399, -1.4001, -0.2017, -0.2257,\n",
      "         0.1735, -1.0885,  0.0337,  0.2305, -2.4502, -0.6648, -0.8209,  0.2909,\n",
      "        -0.9861, -1.0239, -0.9221,  0.1631], grad_fn=<EmbeddingBackward0>)\n",
      "shape: torch.Size([20])\n",
      "word: gonna\n",
      "index: 1\n",
      "embedding: tensor([ 0.8616, -1.0000, -0.9920,  0.3004, -0.1208,  0.1646,  0.9774, -0.1461,\n",
      "        -0.8724,  0.1647, -1.7226, -0.0758,  0.7681,  1.9825, -0.4615, -0.3361,\n",
      "         1.1324,  0.2438,  1.2179,  1.3383], grad_fn=<EmbeddingBackward0>)\n",
      "shape: torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    embed = embeddings(torch.tensor(i))\n",
    "    print(f\"word: {index_to_token[i]}\")\n",
    "    print(f\"index: {i}\")\n",
    "    print(f\"embedding: {embed}\")\n",
    "    print(f\"shape: {embed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9edfe41-2e63-45c6-ba82-56c17f411db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating Context-Target Pairs ("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a90aa05-a166-42ec-8b01-07590a77f6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df271ef6-85c6-4acf-961a-593a63f39ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810d0094-a475-47f2-bd45-65fb83989abb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc322ac-b69d-471b-9c6c-cbb79789c8ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39627ad3-e878-4f40-bfe4-f58ac6c46ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071777f1-e727-44bf-a5f9-360a0da02b27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d3e23b-0fc6-4124-be8f-06e7eb0c3a78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d816e-4e1e-44b6-90a0-1ff5efc3139d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4176d16d-96b3-4212-9ecd-1205b7af0ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2413f89a-dbcc-4604-8c3f-03aac55013b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835e7277-2da3-458f-bbcf-37dbbe7bf4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc10e5af-d8fe-4656-bf1d-7640385e2424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5361c485-bcb7-4f45-bc6c-a126e9e17b06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
