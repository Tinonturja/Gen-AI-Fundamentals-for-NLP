{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6b94e44-459c-485b-a6fb-4fbcba6e4daf",
   "metadata": {},
   "source": [
    "# **Building and Training a Feedforward Neural Network for Language Modeling**\n",
    "\n",
    "Estimated time needed: **60** minutes\n",
    "\n",
    "This project explores the use of Feedforward Neural Networks (FNNs) in language modeling. The primary objective is to build a neural network that learns word relationships and generates meaningful text sequences. The implementation is done using PyTorch, covering key aspects of Natural Language Processing (NLP), such as:\n",
    "* Tokenization & Indexing: Converting text into numerical representations.\n",
    "* Embedding Layers: Mapping words to dense vector representations for efficient learning.\n",
    "* Context-Target Pair Generation (N-grams): Structuring training data for sequence prediction.\n",
    "* Multi-Class Neural Network: Designing a model to predict the next word in a sequence.\n",
    "\n",
    "The training process includes optimizing the model with loss functions and backpropagation techniques to improve accuracy and coherence in text generation. By the end of the project, you will have a working FNN-based language model capable of generating text sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faff11ff-5b70-4014-aa5b-5d5e05d8de34",
   "metadata": {},
   "source": [
    "## STEPS \n",
    "\n",
    "    1. Get the datasets ready\n",
    "    2. Tokenize it (Fragmented the datasets into smaller fractions (words))\n",
    "    3. Create vocabulary from the token (get the indices of the word)\n",
    "    4. Create a function text_pipeline which will take words as input and return indices as output. \n",
    "    5. Create embeddings from the indices by using nn.Embeddings\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a75d855-a3e7-468d-8d96-633f30b69857",
   "metadata": {},
   "source": [
    "## Import required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fbad259-44bf-429f-8db1-031781c5cbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tinonturjamajumder/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/tinonturjamajumder/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "import warnings\n",
    "def warn(*args,**kwargs):\n",
    "    pass\n",
    "warnings.warn = warn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9895fe0-fb45-459b-98a6-e4fddd8698d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "song= \"\"\"We are no strangers to love\n",
    "You know the rules and so do I\n",
    "A full commitments what Im thinking of\n",
    "You wouldnt get this from any other guy\n",
    "I just wanna tell you how Im feeling\n",
    "Gotta make you understand\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Weve known each other for so long\n",
    "Your hearts been aching but youre too shy to say it\n",
    "Inside we both know whats been going on\n",
    "We know the game and were gonna play it\n",
    "And if you ask me how Im feeling\n",
    "Dont tell me youre too blind to see\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Weve known each other for so long\n",
    "Your hearts been aching but youre too shy to say it\n",
    "Inside we both know whats been going on\n",
    "We know the game and were gonna play it\n",
    "I just wanna tell you how Im feeling\n",
    "Gotta make you understand\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6b403f7-b1d8-4a6e-8616-adcb3cb39aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_string(s):\n",
    "    \"\"\"\n",
    "    Preprocess a given string by performing the following steps:\n",
    "    1. Removing anything but letters and digits\n",
    "    2. Removing whitespace\n",
    "    3. Removes all numeric digits\n",
    "    \"\"\"\n",
    "\n",
    "    # Removing all non-word characters (everything except letters and numbers)\n",
    "    # \\w matches any word characters (letters, numbers, and underscores)\n",
    "    # \\s matches any whitespace characters\n",
    "    # ^ inside [] negates the selection, so [^\\w\\s] matches anything that's not a word character or whitespace\n",
    "    s = re.sub(r\"[^\\w\\s]\", '',s)\n",
    "\n",
    "    # removing white spaces\n",
    "    # \\s+ matches one or more whitespace characters.\n",
    "    s = re.sub(r\"\\s+\",'',s)\n",
    "\n",
    "    # removing digits\n",
    "    # \\d matches digits (0-9)\n",
    "    s = re.sub(r\"\\d\",'',s)\n",
    "\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a620950-9d39-4226-99db-32a0574381dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_words(word,tokenizer):\n",
    "    token = tokenizer(word)\n",
    "    tokens = [preprocess_string(w) for w in token if len(w) !=0 and w not in string.punctuation]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f5847c9-0c2a-44a1-879e-5b6c84165b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = preprocess_words(song,word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccd7e041-0baf-4b6e-b38b-be61f7eefa0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We', 'are', 'no', 'strangers', 'to', 'love', 'You', 'know', 'the', 'rules']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b02e922-c84c-4189-ac2b-31c4da557fcd",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91b8dae0-7752-4c7f-868a-ab306f6650b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2a59402-21d5-4ed1-ac9b-b5ff7a854d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizetext(song):\n",
    "    \"\"\"Tokenizes the input text(song) and builds a vocabulary from the tokens\n",
    "\n",
    "    Steps:\n",
    "        1. Tokenizations: The function splits the input text into words and applies a tokenizer function to each word\n",
    "        2. vocabulary building: \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    tokenized_song = map(tokenizer,song.split())\n",
    "\n",
    "    vocab  = build_vocab_from_iterator(tokenized_song,specials = [\"<unk>\"])\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd2b690b-9489-4ee5-87e1-a288eb6d9547",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab= tokenizetext(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fcf8ce9-b42d-4ed7-bc47-259badae2a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', 'gonna', 'you', 'never', 'and', 'tell', 'make', 'say', 'a', 'around']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab.get_itos())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b57f7ef-bcd6-4b16-a1a1-ce3182ed1256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['We', 'are', 'no', 'strangers', 'to', 'love', 'You', 'know', 'the', 'rules'],\n",
       " [0, 58, 70, 74, 25, 69, 0, 20, 31, 72])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:10],vocab(tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37980cf-1fdd-4d65-ae8e-9fab9e6f1e51",
   "metadata": {},
   "source": [
    "A function that converts raw text into indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1e4932c-5926-40af-89b7-39512a64284a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 58, 70, 74, 25, 69, 2, 20, 31, 72]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "text_pipeline(song)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b85c7e4-deb8-4fbb-9e8a-c29c6b3588c6",
   "metadata": {},
   "source": [
    "Convert `index` *-->* `words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0161462e-5261-4743-a450-bb13b332f3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gonna'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_token = vocab.get_itos()\n",
    "index_to_token[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8131cc71-162d-4f81-8fe7-1f5a969bdb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wouldnt',\n",
       " 'what',\n",
       " 'thinking',\n",
       " 'rules',\n",
       " 'no',\n",
       " 'love',\n",
       " 'if',\n",
       " 'full',\n",
       " 'from',\n",
       " 'get']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab.get_stoi())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fcbfcf2-e04f-4f85-bbb1-89461f05c918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = vocab.get_stoi() # stoi is a dictionary basically\n",
    "val['gonna']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce6c9ac-3d77-4818-b2c8-87c166b43aff",
   "metadata": {},
   "source": [
    "## Embedding Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb71ad31-1b71-4d5f-8063-39abe7ebe32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(vocab):\n",
    "    \"\"\" Generating an embedding layer for the given vocabulary,\n",
    "    The embedding layer transforms words into dense vector representation,\n",
    "    allowing the model to learn semantic relationship between words.\n",
    "\n",
    "    Parameters: vocab_size, embedding_dimension\n",
    "\n",
    "    output: nn.Embedding: A PyTorch embedding layer with a specified dimension\"\"\"\n",
    "\n",
    "    embedding_dimension = 20\n",
    "    embedding = nn.Embedding(len(vocab),embedding_dimension)\n",
    "    return embedding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76ac61f9-5a12-4e63-b4b2-862fda639f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.sparse.Embedding"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embedding(vocab)\n",
    "\n",
    "type(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cc28daf-83f3-4e42-837a-0634eb4a1704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8908, -2.2832, -0.1526, -0.1233, -0.6383,  0.4833, -0.8340, -1.2459,\n",
       "         0.1786,  0.5443,  0.9476,  0.2217,  1.5063,  1.1882, -0.4161,  1.3374,\n",
       "         0.4252, -0.1103,  0.7669,  0.1497], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings(torch.tensor(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf224ee8-1f8b-48ad-b56d-05d5f110f04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: <unk>\n",
      "index: 0\n",
      "embedding: tensor([-0.6442, -0.2756,  1.0475, -0.2907, -1.0958,  1.6039, -0.1098,  0.5874,\n",
      "         0.9032, -0.6276,  0.6190, -0.7328, -3.0555,  0.3898,  0.1277, -2.1054,\n",
      "         0.9229,  0.7712,  0.1017,  0.2045], grad_fn=<EmbeddingBackward0>)\n",
      "shape: torch.Size([20])\n",
      "word: gonna\n",
      "index: 1\n",
      "embedding: tensor([ 7.3413e-01, -1.6966e+00, -1.7799e+00, -6.3697e-01,  3.3318e-02,\n",
      "         1.5877e-03,  1.1237e+00,  1.8577e+00,  4.6164e-01,  4.7497e-01,\n",
      "        -3.7971e-01, -5.8951e-01, -6.2694e-02, -1.5155e+00, -2.9917e-01,\n",
      "         8.7756e-02, -7.8534e-01, -1.7775e-01,  1.4298e-01,  3.0548e-01],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "shape: torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    embed = embeddings(torch.tensor(i))\n",
    "    print(f\"word: {index_to_token[i]}\")\n",
    "    print(f\"index: {i}\")\n",
    "    print(f\"embedding: {embed}\")\n",
    "    print(f\"shape: {embed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6598c71a-9c42-4eb2-a420-16bd640bd842",
   "metadata": {},
   "source": [
    "## Generating Context-Target Pairs (n_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7099f4-d18c-41da-b28f-0f5a18d182ed",
   "metadata": {},
   "source": [
    "Organize words with a variable size of context using the following approach: each word is denoted by i, To establish the context, simply subtract 'j'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a90aa05-a166-42ec-8b01-07590a77f6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size for generating n-gram\n",
    "CONTEXT_SIZE  = 2\n",
    "\n",
    "def gengrams(tokens):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    tokens(list): A list of preprocessed word tokens.\n",
    "\n",
    "    Returns:\n",
    "        List: A list of tuples representing n-grams.\n",
    "        Each tuple contains (context_words, target_word)\n",
    "    \"\"\"\n",
    "    ngrams = [\n",
    "    ([tokens[i-j-1] for j in range(CONTEXT_SIZE)], # Context_words\n",
    "    tokens[i] # target words\n",
    "    )\n",
    "        for i in range(CONTEXT_SIZE, len(tokens))\n",
    "    ]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "042ce89c-da0a-4901-aef8-47ea12f40f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "810d0094-a475-47f2-bd45-65fb83989abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram  = gengrams(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffc322ac-b69d-471b-9c6c-cbb79789c8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bi_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39627ad3-e878-4f40-bfe4-f58ac6c46ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_represent(list_length):\n",
    "    for i in range(list_length):\n",
    "        context,target = bi_gram[i]\n",
    "        print(f\" Context: {context}, Target: {target}\")\n",
    "        print(f\"Context index:{vocab([context])},Target index:{vocab([target])}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bace569-68d4-4726-bbde-9e9d21d3b60d",
   "metadata": {},
   "source": [
    "bi_gram returns a list, where each element is a tuple of context words(2 word) and target word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe61bc9-2109-4136-9e37-97d32e305116",
   "metadata": {},
   "source": [
    "## Middle Linear Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb17ed-1e73-47c9-99c9-bdf450e2c122",
   "metadata": {},
   "source": [
    "Aggregate the embeddings of each of these words and then adjust the input size of the subsequent layer accordingly. Then create the next layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4176d16d-96b3-4212-9ecd-1205b7af0ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 20\n",
    "\n",
    "# Next layer\n",
    "linear  = nn.Linear(embedding_dim*CONTEXT_SIZE,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "359e94de-319a-4bce-9688-6a06fee67e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The context would feed to the model after reshaping\n",
    "context,target = bi_gram[0]\n",
    "embeddings = embedding(vocab)\n",
    "my_embedd = embeddings(torch.tensor(vocab(context)))\n",
    "my_embedd.shape # two words in a context, each word has 20 feature embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48d5068-05f4-4fc5-ac0e-733cc3274079",
   "metadata": {},
   "source": [
    "## Reshape the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "835e7277-2da3-458f-bbcf-37dbbe7bf4a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_embedd = my_embedd.reshape(-1,embedding_dim*CONTEXT_SIZE)\n",
    "my_embedd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec00784-5f67-481f-ad5a-4303b8a3b06b",
   "metadata": {},
   "source": [
    "They can now be used as inputs in the next layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5361c485-bcb7-4f45-bc6c-a126e9e17b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3104e-01,  3.5650e-01,  5.9844e-01, -1.2299e-01,  4.0805e-01,\n",
       "          4.5537e-01, -7.3124e-01, -2.9037e-01,  7.0898e-01, -3.5957e-01,\n",
       "          1.3983e+00, -6.0154e-01, -4.5204e-01,  2.0808e-01, -4.2497e-02,\n",
       "         -3.2640e-01,  3.1082e-02, -2.8293e-01, -1.1882e-01, -1.1991e+00,\n",
       "         -3.8555e-01, -1.2894e-01, -1.0768e-01, -3.5823e-01, -1.7548e-01,\n",
       "         -5.9122e-01, -5.7726e-02,  2.5429e-01, -4.3557e-01, -2.2497e-01,\n",
       "         -1.0967e+00,  3.7419e-01,  1.3247e+00,  2.7752e-01, -6.7569e-01,\n",
       "         -1.2808e-01, -8.4872e-01, -7.0129e-01,  4.4884e-01,  3.2655e-01,\n",
       "         -1.5790e-01,  1.2148e-01, -6.2889e-02, -3.6183e-03, -3.6938e-01,\n",
       "          5.1426e-01,  1.8631e-01,  1.2056e-01, -5.4035e-01, -1.0643e+00,\n",
       "          6.5548e-02,  3.9362e-01,  6.8724e-03, -8.9634e-01,  8.2410e-01,\n",
       "          1.2486e-01,  2.2007e-01, -5.2788e-01,  1.3501e-01, -5.7974e-01,\n",
       "          2.2731e-01, -5.1327e-02,  2.1600e-02, -4.5100e-02, -8.2929e-01,\n",
       "          1.0798e+00,  5.2450e-01, -3.2287e-01,  1.9926e-01,  1.7580e-02,\n",
       "         -9.2442e-01,  1.8473e-01, -6.2658e-01,  6.9810e-01, -1.4150e+00,\n",
       "          9.6949e-02, -9.5766e-02,  3.7114e-01, -4.6673e-01, -5.0019e-01,\n",
       "         -1.6580e-02, -2.7363e-01, -2.9262e-01, -3.9929e-01,  5.8526e-01,\n",
       "         -9.8723e-01, -7.4231e-02, -1.0889e-01, -3.4203e-01,  8.6853e-01,\n",
       "         -1.9013e-01,  8.9817e-01, -3.4008e-01, -2.7300e-01, -3.0958e-01,\n",
       "          4.1503e-01, -2.5634e-01,  2.3440e-02,  3.4913e-01,  5.5487e-01,\n",
       "          7.3257e-01, -2.2402e-01, -2.3528e-02,  7.6171e-02, -7.1614e-01,\n",
       "          3.5684e-01,  1.3060e+00,  3.6379e-01, -1.9555e-02,  8.0216e-01,\n",
       "         -6.8678e-02, -3.7513e-01, -2.3866e-01, -5.5461e-01, -8.1028e-01,\n",
       "          1.8523e-04,  3.7513e-02,  9.1759e-01, -3.1039e-01,  8.3918e-01,\n",
       "          2.1673e-02,  1.4068e+00,  6.3819e-01, -5.2694e-01, -4.0869e-01,\n",
       "          3.2027e-01,  5.4081e-01,  2.7865e-01]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(my_embedd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56ea7e4-4417-4bbc-bf2f-0d329e1a5369",
   "metadata": {},
   "source": [
    "## Batch Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57acbfcf-b682-41eb-ac3b-295f79a2244c",
   "metadata": {},
   "source": [
    "Create a batch function to interface with the dataloader. Several adjustments are necessary to handle words that are part of a context in one batch and a predicted word in the following batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46946d54-873f-4ed6-8087-fcb1d3ad25d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 3 # For trigram model\n",
    "BATCH_SIZE = 10 # Number of samples per training batch\n",
    "EMBEDDING_DIM  = 10 # Dimensions of word embedding\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Process a batch of text data into input(context) and output(target) tensors for training a language model.\n",
    "\n",
    "    The function extracts:\n",
    "        - context: A list of word indices representing the context words for each target word.\n",
    "        - target: A list of word indices representing the target word to predict\n",
    "\n",
    "    Parameters:\n",
    "        batch (List): A List of tokenized words (strings).\n",
    "\n",
    "    Returns:\n",
    "        tuple: Two PyTorch tensors: (context_tensor, target_tensor)\n",
    "            - context tensor: Tensor of shape (batch_size - context_size, context_size)\n",
    "              containing the words indices of context words,\n",
    "            - target tensor: Tensor of shape (batch_size - context_size)\n",
    "              containing the word indices of target words.\n",
    "\n",
    "    \"\"\"\n",
    "    batch_size  = len(batch) # get the size of the batch\n",
    "    context,target = [],[] # initialize lists for context \n",
    "\n",
    "    # Loop through the batch, ensuring enough previous words exist for context\n",
    "    for i in range(CONTEXT_SIZE, batch_size):\n",
    "\n",
    "        # Loop through the batch, ensuring enough previous words exist for context\n",
    "        target.append(vocab([batch[i]]))\n",
    "\n",
    "        # Convert the previous context_size words to indices using the vocabulary\n",
    "        context.append(vocab([batch[i-j-1] for j in range(CONTEXT_SIZE)]))\n",
    "\n",
    "    # Convert lists to PyTorch tensors and move them to appropriate device\n",
    "    return torch.tensor(context).to(device),torch.tensor(target).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f83f03e0-5eda-4efa-9c03-313de208b7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "batch_size = 10 , len_token = 56\n",
    "padding = 10-(56%10)  = 10 - 6 = 4\n",
    "the initial 4 token will be added to the end of the token, it is a very common practice while generating music and text\n",
    "\"\"\"\n",
    "padding = BATCH_SIZE - (len(tokens)%BATCH_SIZE)\n",
    "tokken_pad = tokens+tokens[0:padding]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf104b35-d769-4849-b330-9297af9d4087",
   "metadata": {},
   "source": [
    "## Create the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d11812-684a-4857-b449-cdaac1fe6de2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d0b791-5117-4da8-8a12-3a44acbd6afd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a763f081-5c87-40ed-bcf3-971b0e54d46f",
   "metadata": {},
   "source": [
    "## Classificationm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d15e0d1-a394-4f7d-8ba4-bd21d2249e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_function():\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
